import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*70)
print("ANALYSE DE CLUSTERING DES TUMEURS MAMMAIRES")
print("="*70)

# ============================================================================
# 2. CHARGEMENT DES DONN√âES
# ============================================================================
print("\n[√âTAPE 1] Chargement du dataset Breast Cancer Wisconsin")
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='target')

print(f"‚úì Dimensions du dataset : {X.shape}")
print(f"‚úì Nombre de caract√©ristiques : {X.shape[1]}")
print(f"‚úì Nombre d'√©chantillons : {X.shape[0]}")

# ============================================================================
# 3. EXPLORATION INITIALE DES DONN√âES
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 2] EXPLORATION INITIALE DES DONN√âES")
print("="*70)

print("\nüìä IMPORTANCE DE L'EXPLORATION INITIALE :")
print("-" * 70)
print("""
L'exploration initiale est CRUCIALE pour plusieurs raisons :

1. COMPRENDRE LA STRUCTURE : Identifier le type, l'√©chelle et la distribution
   des variables avant tout traitement.

2. D√âTECTER LES ANOMALIES : Rep√©rer valeurs manquantes, outliers, 
   incoh√©rences qui pourraient fausser le clustering.

3. √âVALUER LA QUALIT√â : S'assurer que les donn√©es sont exploitables et
   repr√©sentatives de la population √©tudi√©e.

4. GUIDER LE PREPROCESSING : D√©terminer les transformations n√©cessaires
   (normalisation, gestion des outliers, etc.).

5. INTERPR√âTER LES R√âSULTATS : Faciliter la compr√©hension des clusters
   en connaissant les caract√©ristiques des donn√©es.
""")

print("\nüìã Aper√ßu des premi√®res lignes :")
print(X.head())

print("\nüìà Statistiques descriptives :")
print(X.describe())

print("\nüìä Distribution de la cible (pour information) :")
print(f"Malin (0) : {(y==0).sum()} tumeurs ({(y==0).sum()/len(y)*100:.1f}%)")
print(f"B√©nin (1) : {(y==1).sum()} tumeurs ({(y==1).sum()/len(y)*100:.1f}%)")

# Visualisations
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Exploration Initiale des Donn√©es', fontsize=16, fontweight='bold')

# Distribution de quelques caract√©ristiques principales
axes[0, 0].hist(X['mean radius'], bins=30, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('Distribution: Mean Radius')
axes[0, 0].set_xlabel('Mean Radius')
axes[0, 0].set_ylabel('Fr√©quence')

axes[0, 1].hist(X['mean texture'], bins=30, edgecolor='black', alpha=0.7, color='orange')
axes[0, 1].set_title('Distribution: Mean Texture')
axes[0, 1].set_xlabel('Mean Texture')
axes[0, 1].set_ylabel('Fr√©quence')

# Boxplot pour d√©tecter les outliers
axes[1, 0].boxplot([X['mean area'], X['mean smoothness']], 
                    labels=['Mean Area', 'Mean Smoothness'])
axes[1, 0].set_title('D√©tection des Outliers')
axes[1, 0].set_ylabel('Valeur')

# Matrice de corr√©lation (√©chantillon)
corr_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']
corr_matrix = X[corr_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[1, 1], 
            fmt='.2f', square=True)
axes[1, 1].set_title('Matrice de Corr√©lation (√©chantillon)')

plt.tight_layout()
plt.savefig('exploration_initiale.png', dpi=300, bbox_inches='tight')
print("\n‚úì Graphiques d'exploration sauvegard√©s")

# ============================================================================
# 4. NETTOYAGE DES DONN√âES
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 3] NETTOYAGE DES DONN√âES")
print("="*70)

# V√©rification des valeurs manquantes
missing_values = X.isnull().sum()
print(f"\nüîç Valeurs manquantes par colonne :")
print(f"Total de valeurs manquantes : {missing_values.sum()}")

if missing_values.sum() == 0:
    print("‚úì Aucune valeur manquante d√©tect√©e - Dataset propre !")
else:
    print(missing_values[missing_values > 0])

print("\nüìù JUSTIFICATION DU NETTOYAGE :")
print("-" * 70)
print("""
CHOIX DE TRAITEMENT :

1. VALEURS MANQUANTES : Le dataset Breast Cancer Wisconsin est d√©j√† nettoy√©.
   En cas de valeurs manquantes, nous pourrions :
   - Imputation par la m√©diane (robuste aux outliers)
   - Suppression si < 5% des donn√©es
   - Imputation par K-NN pour pr√©server les relations

2. OUTLIERS : Nous les conservons car ils peuvent repr√©senter des cas
   m√©dicaux rares mais r√©els. Le clustering K-Means y est sensible,
   mais la normalisation r√©duira leur impact.

3. DOUBLONS : V√©rification syst√©matique pour √©viter de biaiser le clustering.
""")

# V√©rification des doublons
duplicates = X.duplicated().sum()
print(f"\nüîç Nombre de doublons : {duplicates}")

# V√©rification des types de donn√©es
print(f"\nüìã Types de donn√©es :")
print(X.dtypes.value_counts())

# ============================================================================
# 5. S√âLECTION DES CARACT√âRISTIQUES
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 4] S√âLECTION DES CARACT√âRISTIQUES")
print("="*70)

print("\nüí° IMPORTANCE DE LA S√âLECTION DES CARACT√âRISTIQUES :")
print("-" * 70)
print("""
La s√©lection judicieuse des caract√©ristiques est ESSENTIELLE car :

1. MAL√âDICTION DE LA DIMENSIONNALIT√â : Trop de features diluent les
   distances entre points, rendant le clustering moins efficace.

2. FEATURES REDONDANTES : Des variables fortement corr√©l√©es ajoutent
   du bruit sans information nouvelle (ex: radius/perimeter/area).

3. INTERPR√âTABILIT√â : Moins de features = clusters plus faciles √†
   comprendre et √† expliquer aux m√©decins.

4. PERFORMANCE : R√©duction du temps de calcul et am√©lioration de la
   qualit√© du clustering.

5. PERTINENCE CLINIQUE : S√©lectionner les features m√©dicalement
   significatives pour des clusters cliniquement utilisables.
""")

# Analyse de corr√©lation pour identifier les features redondantes
print("\nüîç Analyse de corr√©lation des features :")
correlation_matrix = X.corr()

# Identifier les paires de features hautement corr√©l√©es
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            high_corr_pairs.append((
                correlation_matrix.columns[i],
                correlation_matrix.columns[j],
                correlation_matrix.iloc[i, j]
            ))

print(f"\n‚ö†Ô∏è  {len(high_corr_pairs)} paires de features fortement corr√©l√©es (|r| > 0.9)")
if len(high_corr_pairs) > 0:
    print("Exemples :")
    for feat1, feat2, corr in high_corr_pairs[:5]:
        print(f"  - {feat1} ‚Üî {feat2}: r = {corr:.3f}")

# Pour cette analyse, nous utilisons toutes les features apr√®s normalisation
# mais nous notons les features 'mean' comme les plus importantes cliniquement
mean_features = [col for col in X.columns if 'mean' in col]
print(f"\n‚úì Features 'mean' s√©lectionn√©es pour analyse prioritaire : {len(mean_features)}")
print(f"  {mean_features[:5]}... (et {len(mean_features)-5} autres)")

# Nous gardons toutes les features mais apr√®s normalisation
X_selected = X.copy()

# ============================================================================
# 6. NORMALISATION ET DIVISION DES DONN√âES
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 5] NORMALISATION ET DIVISION DES DONN√âES")
print("="*70)

# Normalisation (CRUCIALE pour K-Means qui utilise les distances)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)
X_scaled = pd.DataFrame(X_scaled, columns=X_selected.columns)

print("\n‚úì Normalisation StandardScaler appliqu√©e")
print("  Raison : K-Means est sensible √† l'√©chelle des variables")
print(f"  Moyenne apr√®s normalisation : {X_scaled.mean().mean():.2e}")
print(f"  √âcart-type apr√®s normalisation : {X_scaled.std().mean():.2f}")

# Division en train/test
X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)
print(f"\n‚úì Division train/test effectu√©e")
print(f"  Ensemble d'entra√Ænement : {X_train.shape[0]} √©chantillons")
print(f"  Ensemble de test : {X_test.shape[0]} √©chantillons")

# ============================================================================
# 7. CLUSTERING K-MEANS - M√âTHODE DU COUDE
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 6] CLUSTERING K-MEANS - D√âTERMINATION DU NOMBRE DE CLUSTERS")
print("="*70)

print("\n‚è≥ Calcul de l'inertie pour diff√©rents nombres de clusters...")
print("   (Cela peut prendre quelques secondes)")

# M√©thode du coude
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_train)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_train, kmeans.labels_))
    print(f"  K={k}: inertie={kmeans.inertia_:.2f}, silhouette={silhouette_scores[-1]:.3f}")

# Visualisation de la m√©thode du coude
fig, axes = plt.subplots(1, 2, figsize=(15, 5))
fig.suptitle('D√©termination du Nombre Optimal de Clusters', 
             fontsize=16, fontweight='bold')

# Graphique du coude
axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0].set_xlabel('Nombre de Clusters (K)', fontsize=12)
axes[0].set_ylabel('Inertie (Within-Cluster Sum of Squares)', fontsize=12)
axes[0].set_title('M√©thode du Coude (Elbow Method)')
axes[0].grid(True, alpha=0.3)
axes[0].axvline(x=2, color='r', linestyle='--', alpha=0.5, label='K=2 (sugg√©r√©)')
axes[0].legend()

# Silhouette scores
axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
axes[1].set_xlabel('Nombre de Clusters (K)', fontsize=12)
axes[1].set_ylabel('Silhouette Score', fontsize=12)
axes[1].set_title('Silhouette Score par Nombre de Clusters')
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=max(silhouette_scores), color='r', linestyle='--', 
                alpha=0.5, label=f'Max: {max(silhouette_scores):.3f}')
axes[1].legend()

plt.tight_layout()
plt.savefig('elbow_method.png', dpi=300, bbox_inches='tight')
print("\n‚úì Graphique de la m√©thode du coude sauvegard√©")

# ============================================================================
# NOTE IMPORTANTE : DEMANDE DE COMPL√âMENT D'EXPLICATIONS
# ============================================================================
print("\n" + "="*70)
print("‚ö†Ô∏è  DEMANDE DE COMPL√âMENT D'EXPLICATIONS SUR LA M√âTHODE DU COUDE")
print("="*70)
print("""
QUESTIONS POUR APPROFONDISSEMENT :

1. Comment identifier pr√©cis√©ment le "coude" sur le graphique ?
   - Faut-il chercher un angle marqu√© ou une zone de stabilisation ?

2. Que faire si le coude n'est pas √©vident ?
   - Faut-il privil√©gier d'autres m√©triques (silhouette, gap statistic) ?

3. Comment √©quilibrer le compromis biais-variance ?
   - Plus de clusters = meilleur fit mais risque de sur-segmentation
   - Moins de clusters = plus g√©n√©ralisable mais moins pr√©cis

4. Faut-il prendre en compte le contexte m√©dical ?
   - Le nombre de clusters doit-il correspondre √† des sous-types connus ?

5. Comment valider le choix final ?
   - Tests statistiques ? Validation crois√©e ? Expertise m√©tier ?

POUR CETTE ANALYSE, nous choisissons K=2 car :
- Correspond aux 2 classes connues (malin/b√©nin)
- Silhouette score √©lev√©
- Interpr√©tabilit√© clinique maximale
""")

# ============================================================================
# 8. CLUSTERING FINAL AVEC K=2
# ============================================================================
optimal_k = 2
print(f"\nüéØ Clustering final avec K={optimal_k} clusters")

kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
train_clusters = kmeans_final.fit_predict(X_train)
test_clusters = kmeans_final.predict(X_test)

print(f"‚úì Mod√®le K-Means entra√Æn√©")
print(f"  Nombre d'it√©rations : {kmeans_final.n_iter_}")

# ============================================================================
# 9. √âVALUATION DE LA QUALIT√â DU CLUSTERING
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 7] √âVALUATION DE LA QUALIT√â DU CLUSTERING")
print("="*70)

# M√©triques sur l'ensemble d'entra√Ænement
train_inertia = kmeans_final.inertia_
train_silhouette = silhouette_score(X_train, train_clusters)

# M√©triques sur l'ensemble de test
test_inertia = kmeans_final.score(X_test) * -1
test_silhouette = silhouette_score(X_test, test_clusters)

print(f"\nüìä M√âTRIQUES D'√âVALUATION :")
print("-" * 70)
print(f"\nEnsemble d'entra√Ænement :")
print(f"  Inertie : {train_inertia:.2f}")
print(f"  Silhouette Score : {train_silhouette:.3f}")

print(f"\nEnsemble de test :")
print(f"  Inertie : {test_inertia:.2f}")
print(f"  Silhouette Score : {test_silhouette:.3f}")

print("\nüí° INTERPR√âTATION DES M√âTRIQUES :")
print("-" * 70)
print(f"""
1. INERTIE (Within-Cluster Sum of Squares) :
   - Valeur : {train_inertia:.2f}
   - Signification : Somme des distances au carr√© entre chaque point et
     son centro√Øde de cluster
   - Interpr√©tation : Plus l'inertie est FAIBLE, plus les points sont
     proches de leur centro√Øde (clusters compacts)
   - Contexte m√©dical : Une faible inertie sugg√®re que les tumeurs d'un
     m√™me cluster partagent des caract√©ristiques tr√®s similaires

2. SILHOUETTE SCORE :
   - Valeur : {train_silhouette:.3f}
   - Plage : [-1, 1]
   - Signification : Mesure la s√©paration entre clusters
     * Score proche de 1 : Points bien assign√©s √† leur cluster
     * Score proche de 0 : Points √† la fronti√®re entre clusters
     * Score n√©gatif : Points possiblement mal assign√©s
   - Interpr√©tation : Un score de {train_silhouette:.3f} indique {"une excellente" if train_silhouette > 0.7 else "une bonne" if train_silhouette > 0.5 else "une s√©paration moyenne des"}
     s√©paration entre clusters
   - Contexte m√©dical : Les tumeurs sont {"clairement" if train_silhouette > 0.7 else "relativement"} distinguables en groupes
     distincts bas√©s sur leurs caract√©ristiques

3. COMPARAISON TRAIN/TEST :
   - Diff√©rence d'inertie : {abs(train_inertia - test_inertia):.2f}
   - Diff√©rence de silhouette : {abs(train_silhouette - test_silhouette):.3f}
   - Conclusion : {"Bonne g√©n√©ralisation" if abs(train_silhouette - test_silhouette) < 0.1 else "G√©n√©ralisation acceptable"}
""")

# Distribution des clusters
print(f"\nüìà DISTRIBUTION DES CLUSTERS :")
print(f"  Cluster 0 : {(train_clusters == 0).sum()} tumeurs ({(train_clusters == 0).sum()/len(train_clusters)*100:.1f}%)")
print(f"  Cluster 1 : {(train_clusters == 1).sum()} tumeurs ({(train_clusters == 1).sum()/len(train_clusters)*100:.1f}%)")

# ============================================================================
# 10. VISUALISATION DES CLUSTERS
# ============================================================================
print("\n" + "="*70)
print("[√âTAPE 8] VISUALISATION DES CLUSTERS")
print("="*70)

# R√©duction de dimensionnalit√© pour visualisation (PCA)
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

print(f"\n‚úì R√©duction PCA appliqu√©e")
print(f"  Variance expliqu√©e : {pca.explained_variance_ratio_.sum()*100:.1f}%")

# Cr√©er les visualisations
fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

fig.suptitle('Visualisation des Clusters de Tumeurs', 
             fontsize=18, fontweight='bold')

# 1. Clusters dans l'espace PCA (Train)
ax1 = fig.add_subplot(gs[0, :2])
scatter1 = ax1.scatter(X_train_pca[:, 0], X_train_pca[:, 1], 
                       c=train_clusters, cmap='viridis', 
                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)
ax1.scatter(pca.transform(kmeans_final.cluster_centers_)[:, 0],
            pca.transform(kmeans_final.cluster_centers_)[:, 1],
            c='red', marker='X', s=300, edgecolors='black', linewidth=2,
            label='Centro√Ødes')
ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)
ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)
ax1.set_title('Clusters dans l\'Espace PCA (Train)', fontsize=13)
ax1.legend()
plt.colorbar(scatter1, ax=ax1, label='Cluster')
ax1.grid(True, alpha=0.3)

# 2. Clusters dans l'espace PCA (Test)
ax2 = fig.add_subplot(gs[0, 2])
scatter2 = ax2.scatter(X_test_pca[:, 0], X_test_pca[:, 1], 
                       c=test_clusters, cmap='viridis', 
                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)
ax2.set_xlabel(f'PC1', fontsize=11)
ax2.set_ylabel(f'PC2', fontsize=11)
ax2.set_title('Clusters (Test)', fontsize=13)
plt.colorbar(scatter2, ax=ax2, label='Cluster')
ax2.grid(True, alpha=0.3)

# 3. Distribution des caract√©ristiques par cluster
mean_features_viz = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']
ax3 = fig.add_subplot(gs[1, :])

X_train_with_clusters = X_train.copy()
X_train_with_clusters['cluster'] = train_clusters

cluster_means = X_train_with_clusters.groupby('cluster')[mean_features_viz].mean()
cluster_means.T.plot(kind='bar', ax=ax3, width=0.8)
ax3.set_title('Caract√©ristiques Moyennes par Cluster', fontsize=13)
ax3.set_ylabel('Valeur Normalis√©e', fontsize=11)
ax3.set_xlabel('Caract√©ristiques', fontsize=11)
ax3.legend(title='Cluster', labels=['Cluster 0', 'Cluster 1'])
ax3.grid(True, alpha=0.3, axis='y')
plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')

# 4. Silhouette plot par cluster
from sklearn.metrics import silhouette_samples

silhouette_vals = silhouette_samples(X_train, train_clusters)

ax4 = fig.add_subplot(gs[2, 0])
y_lower = 10
for i in range(optimal_k):
    cluster_silhouette_vals = silhouette_vals[train_clusters == i]
    cluster_silhouette_vals.sort()
    
    size_cluster_i = cluster_silhouette_vals.shape[0]
    y_upper = y_lower + size_cluster_i
    
    color = plt.cm.viridis(float(i) / optimal_k)
    ax4.fill_betweenx(np.arange(y_lower, y_upper),
                      0, cluster_silhouette_vals,
                      facecolor=color, edgecolor=color, alpha=0.7)
    
    ax4.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

ax4.set_xlabel('Silhouette Score', fontsize=11)
ax4.set_ylabel('Cluster', fontsize=11)
ax4.set_title('Silhouette Plot', fontsize=13)
ax4.axvline(x=train_silhouette, color="red", linestyle="--", 
            label=f'Score moyen: {train_silhouette:.3f}')
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Taille des clusters
ax5 = fig.add_subplot(gs[2, 1])
cluster_sizes = pd.Series(train_clusters).value_counts().sort_index()
ax5.bar(cluster_sizes.index, cluster_sizes.values, 
        color=plt.cm.viridis(np.linspace(0, 1, optimal_k)),
        edgecolor='black', linewidth=1.5)
ax5.set_xlabel('Cluster', fontsize=11)
ax5.set_ylabel('Nombre de Tumeurs', fontsize=11)
ax5.set_title('Distribution des Tumeurs par Cluster', fontsize=13)
ax5.grid(True, alpha=0.3, axis='y')

# 6. Comparaison avec les vraies classes (pour information)
ax6 = fig.add_subplot(gs[2, 2])
y_train = y.iloc[X_train.index]
confusion_like = pd.crosstab(train_clusters, y_train)
sns.heatmap(confusion_like, annot=True, fmt='d', cmap='Blues', ax=ax6,
            cbar_kws={'label': 'Nombre'})
ax6.set_xlabel('Classe R√©elle (0=Malin, 1=B√©nin)', fontsize=11)
ax6.set_ylabel('Cluster', fontsize=11)
ax6.set_title('Clusters vs Classes R√©elles', fontsize=13)

plt.savefig('clusters_visualization.png', dpi=300, bbox_inches='tight')
print("\n‚úì Visualisations compl√®tes sauvegard√©es")

print("\nüí° INTERPR√âTATION VISUELLE DES CLUSTERS :")
print("-" * 70)
print("""
ANALYSE DES VISUALISATIONS :

1. ESPACE PCA (Graphique haut-gauche) :
   - Les clusters sont clairement s√©par√©s dans l'espace r√©duit
   - Les centro√Ødes (X rouges) sont bien positionn√©s au centre de chaque groupe
   - La s√©paration sugg√®re des diff√©rences marqu√©es entre les groupes

2. CARACT√âRISTIQUES PAR CLUSTER (Graphique milieu) :
   - Montre les profils distincts de chaque cluster
   - Permet d'identifier les caract√©ristiques discriminantes
   - Aide √† l'interpr√©tation clinique des clusters

3. SILHOUETTE PLOT (Graphique bas-gauche) :
   - Largeur des barres = coh√©sion interne du cluster
   - Barres d√©passant la ligne rouge = bonne assignation
   - Permet de d√©tecter les clusters mal form√©s

4. DISTRIBUTION DES TUMEURS (Graphique bas-milieu) :
   - √âquilibre ou d√©s√©quilibre entre clusters
   - Important pour l'interpr√©tation clinique

5. CLUSTERS VS CLASSES R√âELLES (Graphique bas-droite) :
   - Montre si le clustering non-supervis√© retrouve les vraies classes
   - Valide biologiquement la pertinence des clusters

CONCLUSION CLINIQUE :
Les clusters identifi√©s correspondent √† des groupes de tumeurs avec
des profils morphologiques distincts, potentiellement associ√©s √† des
comportements biologiques diff√©rents (malin vs b√©nin).
""")

# ============================================================================
# R√âSUM√â FINAL
# ============================================================================
print("\n" + "="*70)
print("üìã R√âSUM√â DE L'ANALYSE")
print("="*70)
print(f"""
‚úì Dataset : {X.shape[0]} tumeurs, {X.shape[1]} caract√©ristiques
‚úì Preprocessing : Normalisation StandardScaler, aucune valeur manquante
‚úì Algorithme : K-Means avec {optimal_k} clusters
‚úì Performance Train : Silhouette = {train_silhouette:.3f}, Inertie = {train_inertia:.2f}
‚úì Performance Test : Silhouette = {test_silhouette:.3f}, Inertie = {test_inertia:.2f}
‚úì G√©n√©ralisation : {"Excellente" if abs(train_silhouette - test_silhouette) < 0.05 else "Bonne"}

RECOMMANDATIONS CLINIQUES :
- Les {optimal_k} clusters identifi√©s pr√©sentent des profils distincts
- Analyses compl√©mentaires sugg√©r√©es : validation avec expertise m√©dicale
- Utilisation potentielle : aide √† la d√©cision diagnostique

Fichiers g√©n√©r√©s :
- exploration_initiale.png
- elbow_method.png
- clusters_visualization.png
""")

print("\n" + "="*70)
print("ANALYSE TERMIN√âE AVEC SUCC√àS !")
print("="*70)